<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Abdullah Alghamdi - Coding Logs</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #f4f7fa;
      color: #333;
    }

    .caption {
      font-size: 28px;
      font-weight: bold;
      text-align: center;
      margin-bottom: 40px;
      color: #1a237e;
    }

    .chapter {
      background-color: #ffffff;
      border: 1px solid #dce1ea;
      border-radius: 10px;
      padding: 20px;
      margin-bottom: 30px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.05);
    }

    .chapter-title {
      font-size: 22px;
      font-weight: bold;
      color: #0d47a1;
      margin-bottom: 8px;
    }

    .chapter-date {
      font-size: 14px;
      font-weight: bold;
      color: #555;
      margin-bottom: 20px;
    }

    .description {
      font-size: 16px;
      line-height: 1.6;
    }

    strong {
      color: #263238;
    }
  </style>
</head>
<body>

  <div class="caption">Hello, I am Abdullah Alghamdi</div>

  <!-- New Chapter - June 20, 2025 -->
  <div class="chapter">
    <div class="chapter-title">Pose Estimation and Video Analysis for Sign Language Research</div>
    <div class="chapter-date">June 20, 2025</div>
    <div class="description">
      Today I finally fixed my code with help from Joe, and I was able to finish processing all the videos. It took some time, but now everything is working as expected. Dr. Raja told me to get ready for the presentation on Friday, so I’ve started thinking about what I should say.<br><br>
      I’ll need to show the code that both AJ and I worked on. Since AJ won’t be coming, I’ll have to take over his part too, which means more responsibility during the 15-minute presentation. I’ll do my best to explain both of our roles clearly.<br><br>
      Right now I’m planning to include a quick summary of our project goals, a live or recorded demo of how our code works, and maybe show some of the results we got from the videos. I want to keep it clear, not too technical, but still show the important points we worked hard on.<br><br>
      I might need to run through the presentation once or twice to make sure the timing is right and I feel confident. Let me know if there’s anything I should add or focus on more.
    </div>
  </div>

  <!-- Chapter 1 -->
  <div class="chapter">
    <div class="chapter-title">Weekend Coding Log</div>
    <div class="chapter-date">June 13, 2025</div>
    <div class="description">
      Over the weekend, I encountered several challenges while working on my code related to tracking sign language gestures using MediaPipe. The code was not functioning properly, and I spent several hours debugging it, unsure whether the issue was in the logic, the input video, or the model itself.<br><br>
      Eventually, I discovered that the root cause was missing model data—specifically, the keypoint models for the face and hands had not been downloaded. Once I located and installed the required MediaPipe assets, the code began to function correctly. This experience reminded me how crucial it is to check model dependencies early, especially when using holistic tracking systems that rely on multiple submodels.<br><br>
      While solving this issue felt like a win, I also found myself confused about the direction of my next steps. I wasn’t sure whether I should move forward by trying to <strong>cut video segments where signs stop and start</strong>, or whether I should focus more narrowly on <strong>fingerspelling detection</strong>, which seems to require finer control and precision.<br><br>
      This uncertainty is part of the process, but it highlights how important it is to balance technical troubleshooting with bigger-picture planning. I’ll continue experimenting and exploring which direction aligns best with the goals of the ASL tracking project.
    </div>
  </div>

  <!-- Chapter 2 -->
  <div class="chapter">
    <div class="chapter-title">Deep Learning Approaches to Sign Language Recognition</div>
    <div class="chapter-date">June 6, 2025</div>
    <div class="description">
      <strong>Weekend Coding Log:</strong><br><br>
      Two key deep learning methods for sign language recognition are pose keypoint sequences and multi-modal classification. The pose-based approach uses skeletal points (hands, arms, body) fed into models like LSTMs or Transformers to detect signs. Ko et al. (2019) showed this method works well for Korean Sign Language and is efficient and privacy-friendly. However, it often misses facial expressions and detailed handshapes, especially when keypoints are poorly tracked. Multi-modal approaches, such as that by Saunders et al. (2020), combine video, keypoints, facial data, and optical flow for richer recognition. These models perform better in expressive communication but require more data, computation, and precise video quality. Overall, pose-based models are simpler and scalable, while multi-modal systems offer deeper understanding at higher cost.
    </div>
  </div>

  <!-- Chapter 3 -->
  <div class="chapter">
    <div class="chapter-title">Sign Language Detection Using Human Pose Estimation</div>
    <div class="chapter-date">May 30, 2025</div>
    <div class="description">
      <!-- Add your May 30 content here if ready -->
      (Entry coming soon...)
    </div>
  </div>

</body>
</html>
