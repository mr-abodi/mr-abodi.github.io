<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Abdullah Alghamdi - Coding Logs</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #f4f7fa;
      color: #333;
    }

    .caption {
      font-size: 28px;
      font-weight: bold;
      text-align: center;
      margin-bottom: 40px;
      color: #1a237e;
    }

    .chapter {
      background-color: #ffffff;
      border: 1px solid #dce1ea;
      border-radius: 10px;
      padding: 20px;
      margin-bottom: 30px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.05);
    }

    .chapter-title {
      font-size: 22px;
      font-weight: bold;
      color: #0d47a1;
      margin-bottom: 8px;
    }

    .chapter-date {
      font-size: 14px;
      font-weight: bold;
      color: #555;
      margin-bottom: 20px;
    }

    .description {
      font-size: 16px;
      line-height: 1.6;
    }

    strong {
      color: #263238;
    }
  </style>
</head>
<body>

  <div class="caption">Hello, I am Abdullah Alghamdi</div>

  <!-- New Chapter - June 27, 2025 -->
  <div class="chapter">
    <div class="chapter-title">ASL Video Analysis – Final Coding and Presentation Report</div>
    <div class="chapter-date">June 27, 2025</div>
    <div class="description">
      I finished working on the code for our ASL video analysis project. I used MediaPipe to track body pose and hand movements, and I made sure the code ran without errors. I cleaned up the script and tested it on both old videos and high-quality ones. I also included AJ’s part of the code and made sure everything worked well together.
      AJ was not available that day, but Joe helped me a lot with fixing and reviewing the code. With his support, I was able to complete everything on time.
      I also prepared the presentation. I made slides that explained the project, including the problems we faced with low-quality and tilted videos. I talked about how we used MediaPipe Holistic to compare poses and hand signs across different videos. I explained how we got our results and what challenges we had, especially with missing points and translation issues.
      During the presentation, I spoke for about 15 minutes, covering both my work and AJ’s. I answered questions and shared what I learned. I’m proud that I was able to handle both the coding and the presentation with Joe’s support.
      Overall, it was a productive day, and I’m happy with what I accomplished.
    </div>
  </div>
  
  <!-- New Chapter - June 20, 2025 -->
  <div class="chapter">
    <div class="chapter-title">Pose Estimation and Video Analysis for Sign Language Research</div>
    <div class="chapter-date">June 20, 2025</div>
    <div class="description">
      Today I finally fixed my code with help from Joe, and I was able to finish processing all the videos. It took some time, but now everything is working as expected. Dr. Raja told me to get ready for the presentation on Friday, so I’ve started thinking about what I should say.<br><br>
      I’ll need to show the code that both AJ and I worked on. Since AJ won’t be coming, I’ll have to take over his part too, which means more responsibility during the 15-minute presentation. I’ll do my best to explain both of our roles clearly.<br><br>
      Right now I’m planning to include a quick summary of our project goals, a live or recorded demo of how our code works, and maybe show some of the results we got from the videos. I want to keep it clear, not too technical, but still show the important points we worked hard on.<br><br>
      I might need to run through the presentation once or twice to make sure the timing is right and I feel confident. Let me know if there’s anything I should add or focus on more.
    </div>
  </div>

  <!-- Chapter 1 -->
  <div class="chapter">
    <div class="chapter-title">Weekend Coding Log</div>
    <div class="chapter-date">June 13, 2025</div>
    <div class="description">
      Over the weekend, I encountered several challenges while working on my code related to tracking sign language gestures using MediaPipe. The code was not functioning properly, and I spent several hours debugging it, unsure whether the issue was in the logic, the input video, or the model itself.
      Eventually, I discovered that the root cause was missing model data—specifically, the keypoint models for the face and hands had not been downloaded. Once I located and installed the required MediaPipe assets, the code began to function correctly. This experience reminded me how crucial it is to check model dependencies early, especially when using holistic tracking systems that rely on multiple submodels.
      While solving this issue felt like a win, I also found myself confused about the direction of my next steps. I wasn’t sure whether I should move forward by trying to cut video segments where signs stop and start, or whether I should focus more narrowly on fingerspelling detection, which seems to require finer control and precision.
      This uncertainty is part of the process, but it highlights how important it is to balance technical troubleshooting with bigger-picture planning. I’ll continue experimenting and exploring which direction aligns best with the goals of the ASL tracking project.
    </div>
  </div>

  <!-- Chapter 2 -->
  <div class="chapter">
    <div class="chapter-title">Deep Learning Approaches to Sign Language Recognition</div>
    <div class="chapter-date">June 6, 2025</div>
    <div class="description">
      Two key deep learning methods for sign language recognition are pose keypoint sequences and multi-modal classification. The pose-based approach uses skeletal points (hands, arms, body) fed into models like LSTMs or Transformers to detect signs. Ko et al. (2019) showed this method works well for Korean Sign Language and is efficient and privacy-friendly. However, it often misses facial expressions and detailed handshapes, especially when keypoints are poorly tracked. Multi-modal approaches, such as that by Saunders et al. (2020), combine video, keypoints, facial data, and optical flow for richer recognition. These models perform better in expressive communication but require more data, computation, and precise video quality. Overall, pose-based models are simpler and scalable, while multi-modal systems offer deeper understanding at higher cost.
    </div>
  </div>

  <!-- Chapter 3 -->
  <div class="chapter">
    <div class="chapter-title">Sign Language Detection Using Human Pose Estimation</div>
    <div class="chapter-date">May 30, 2025</div>
    <div class="description">
      <!-- Add your May 30 content here if ready -->
      This is what I research for ASL data: Sign Language Detection Using Human Pose Estimation. The paper “Real-Time Sign Language Detection Using Human Pose Estimation” by Amit Moryossef and team explores detecting whether someone is signing in real time—not recognizing signs, just detecting activity. It's important for making video calls more accessible for Deaf users by showing their video when they sign.
      They used the Public DGS Corpus with OpenPose tracking and neural networks (LSTM). The system is fast and accurate, even on standard CPUs. However, it still has limitations—like recognizing only one signer at a time and needing diverse training data.
      This research is a big step for improving digital accessibility and helps with efforts like Gallaudet’s ASL video tracking projects.

    </div>
  </div>

</body>
</html>
